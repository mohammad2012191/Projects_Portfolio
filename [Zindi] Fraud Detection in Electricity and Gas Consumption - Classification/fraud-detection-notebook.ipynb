{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Electricity and Gas Consumption - Fraud Detection\n",
    "# By Mohamed Eltayeb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the Features Importances\n",
    "def plotImp(model, X , num = 30, fig_size = (60, 30)):\n",
    "    feature_imp = pd.DataFrame({'Value':model.feature_importances_,'Feature':X.columns})\n",
    "    plt.figure(figsize=fig_size)\n",
    "    sns.set(font_scale = 5)\n",
    "    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", \n",
    "                                                        ascending=False)[0:num])\n",
    "    plt.title('Catboost Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cb_importances-01.png')\n",
    "    plt.show()\n",
    "    return feature_imp.sort_values(by=\"Value\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce Memory Usage\n",
    "def reduce_memory_usage(df):\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype.name\n",
    "        if ((col_type != 'datetime64[ns]') & (col_type != 'category')):\n",
    "            if (col_type != 'object'):\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "\n",
    "                if str(col_type)[:3] == 'int':\n",
    "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                        df[col] = df[col].astype(np.int64)\n",
    "\n",
    "                else:\n",
    "                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                        df[col] = df[col].astype(np.float16)\n",
    "                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "                    else:\n",
    "                        pass\n",
    "            else:\n",
    "                df[col] = df[col].astype('category')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the training and testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"client_train.csv\",low_memory=False)\n",
    "test_df = pd.read_csv(\"client_test.csv\",low_memory=False)\n",
    "invoice_train = pd.read_csv(\"invoice_train.csv\",low_memory=False)\n",
    "invoice_test = pd.read_csv(\"invoice_test.csv\",low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Time-related Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoice_train = invoice_train.sort_values(['client_id','invoice_date']).reset_index(drop=True) \n",
    "invoice_test = invoice_test.sort_values(['client_id','invoice_date']).reset_index(drop=True)\n",
    "\n",
    "invoice_train['invoice_date'] = invoice_train['invoice_date'].astype('datetime64[ns]')\n",
    "invoice_test['invoice_date'] = invoice_test['invoice_date'].astype('datetime64[ns]')\n",
    "train_df['creation_date'] = train_df['creation_date'].astype('datetime64[ns]')\n",
    "test_df['creation_date'] = test_df['creation_date'].astype('datetime64[ns]')\n",
    "\n",
    "for dataset in (invoice_train,invoice_test):\n",
    "        Date = 'invoice_date'\n",
    "        dataset[f'{Date}_Date_Int'] = dataset[Date].astype(np.int64) * 1e-9\n",
    "        dataset[f'{Date}_Day'] = dataset[Date].dt.day\n",
    "        dataset[f'{Date}_Month'] = dataset[Date].dt.month\n",
    "        dataset[f'{Date}_Year'] = dataset[Date].dt.year         \n",
    "        dataset.drop(Date,inplace=True,axis=1)\n",
    "        \n",
    "for dataset in (train_df,test_df):\n",
    "        Date = 'creation_date'\n",
    "        dataset[f'{Date}_Date_Int'] = dataset[Date].astype(np.int64) * 1e-9\n",
    "        dataset[f'{Date}_Day'] = dataset[Date].dt.day\n",
    "        dataset[f'{Date}_Month'] = dataset[Date].dt.month\n",
    "        dataset[f'{Date}_Year'] = dataset[Date].dt.year         \n",
    "        dataset.drop(Date,inplace=True,axis=1)\n",
    "    \n",
    "invoice_train = reduce_memory_usage(invoice_train)\n",
    "invoice_test = reduce_memory_usage(invoice_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correct Some errors in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoice_train['counter_statue'] = invoice_train['counter_statue'].map({0:0,1:1,2:2,3:3,4:4,5:5,769:5,'0':0,'5':5,'1':1,'4':4,'A':0,618:5,269375:5,46:5,420:5})\n",
    "for dataset in [invoice_train,invoice_test]:\n",
    "    dataset['counter_statue'] = dataset['counter_statue'].astype(str)\n",
    "train_df['target'] = train_df['target'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store The IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = test_df['client_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Features Name to be Used For Aggregations Later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aggs_based = ['client_id']\n",
    "Aggs_num = ['consommation_level_1','consommation_level_2','consommation_level_3',\n",
    "            'consommation_level_4','months_number']\n",
    "Aggs_cat = ['reading_remarque','counter_coefficient','tarif_type',\n",
    "            'counter_number','counter_statue','counter_code',\n",
    "            'old_index','new_index','counter_type','invoice_date_Date_Int',\n",
    "            'invoice_date_Day','invoice_date_Month','invoice_date_Year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference Between New_index and Old_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [invoice_train,invoice_test]:\n",
    "    dataset['NewDiffOld'] = dataset['new_index'] - dataset['old_index']\n",
    "Aggs_cat += ['NewDiffOld']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regions Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [train_df,test_df]:\n",
    "    dataset['region_bins'] = dataset['region'].apply(lambda x: 1 if x<=100 else 3 if x>=300 else 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Months since the account created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['MonthSinceAccounCreationt'] = (2022 - train_df['creation_date_Year'])*12 - train_df['creation_date_Month']\n",
    "test_df['MonthSinceAccounCreation'] = (2022 - test_df['creation_date_Year'])*12 - test_df['creation_date_Month']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoice_train['counter_code_number_add'] = invoice_train['counter_code'] + invoice_train['counter_number'] \n",
    "invoice_train['counter_code_number_sub'] = invoice_train['counter_code'] - invoice_train['counter_number'] \n",
    "invoice_train['counter_code_number_prod'] = invoice_train['counter_code'] * invoice_train['counter_number'] \n",
    "invoice_train['counter_code_number_div'] = invoice_train['counter_code'] / invoice_train['counter_number'] \n",
    "\n",
    "invoice_test['counter_code_number_add'] = invoice_test['counter_code'] + invoice_test['counter_number'] \n",
    "invoice_test['counter_code_number_sub'] = invoice_test['counter_code'] - invoice_test['counter_number'] \n",
    "invoice_test['counter_code_number_prod'] = invoice_test['counter_code'] * invoice_test['counter_number'] \n",
    "invoice_test['counter_code_number_div'] = invoice_test['counter_code'] / invoice_test['counter_number'] \n",
    "\n",
    "Aggs_num += ['counter_code_number_add','counter_code_number_sub',\n",
    "             'counter_code_number_prod','counter_code_number_div']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding for Some of the Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df = pd.concat([invoice_train, invoice_test])\n",
    "for f in (Aggs_cat):\n",
    "    le.fit(df[f])\n",
    "    invoice_train[f] = le.transform(invoice_train[f])\n",
    "    invoice_test[f] = le.transform(invoice_test[f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Aggregations (Numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Agg(Feature):\n",
    "    for client in (train_df,test_df):\n",
    "        dataset = invoice_train if client.equals(train_df) else invoice_test\n",
    "        for feat_1 in Aggs_based:\n",
    "            client[f'{Feature}_Agg_{feat_1}_mean'] = client[feat_1].map(dict(dataset.groupby(feat_1)[Feature].mean()))\n",
    "            client[f'{Feature}_Agg_{feat_1}_median'] = client[feat_1].map(dict(dataset.groupby(feat_1)[Feature].median()))\n",
    "            client[f'{Feature}_Agg_{feat_1}_std'] = client[feat_1].map(dict(dataset.groupby(feat_1)[Feature].std()))\n",
    "            client[f'{Feature}_Agg_{feat_1}_min'] = client[feat_1].map(dict(dataset.groupby(feat_1)[Feature].min()))\n",
    "            client[f'{Feature}_Agg_{feat_1}_max'] = client[feat_1].map(dict(dataset.groupby(feat_1)[Feature].max()))\n",
    "            client[f'{Feature}_Agg_{feat_1}_sum'] = client[feat_1].map(dict(dataset.groupby(feat_1)[Feature].sum()))\n",
    "            client[f'{Feature}_Agg_{feat_1}_range'] = client[f'{Feature}_Agg_{feat_1}_max'] - client[f'{Feature}_Agg_{feat_1}_min']                \n",
    "for feat in tqdm(Aggs_num + Aggs_cat):         \n",
    "    Agg(feat)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Aggregations (Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Agg(Feature):\n",
    "    for client in (train_df,test_df):\n",
    "        dataset = invoice_train if client.equals(train_df) else invoice_test\n",
    "        for feat_1 in Aggs_based:\n",
    "            client[f'{Feature}_Agg_{feat_1}_mode'] = client[feat_1].map(dict(dataset.groupby(feat_1)[Feature].agg(lambda x: pd.Series.mode(x)[0])))\n",
    "            client[f'{Feature}_Agg_{feat_1}_nunique'] = client[feat_1].map(dict(dataset.groupby(feat_1)[Feature].nunique()))\n",
    "                \n",
    "for feat in tqdm(Aggs_cat):         \n",
    "    Agg(feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Client_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [train_df,test_df]:\n",
    "    dataset.drop('client_id',inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Label Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = list(train_df.select_dtypes(include=['object','category']).columns)\n",
    "le = LabelEncoder()\n",
    "df = pd.concat([train_df, test_df])\n",
    "for f in feats:\n",
    "    print(f)\n",
    "    le.fit(df[f])\n",
    "    train_df[f] = le.transform(train_df[f])\n",
    "    test_df[f] = le.transform(test_df[f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = ['region','disrict']\n",
    "df = pd.concat([train_df,test_df])\n",
    "for feat in feats:\n",
    "    Names = [f'{feat}_{x}' for x in df[feat].value_counts().keys().sort_values()]\n",
    "    OHE_cols = pd.DataFrame(pd.get_dummies(df[feat]).values,index = df.index, columns = Names)\n",
    "    df = pd.concat([df,OHE_cols],axis=1)\n",
    "    \n",
    "train_df = df[:train_df.shape[0]]\n",
    "test_df = df[train_df.shape[0]:]\n",
    "test_df.drop('target',inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Duplicates and Constants Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Features Before Dropping: ', train_df.shape)\n",
    "#Drop Duplicate Features\n",
    "cols = train_df.columns\n",
    "dup = []\n",
    "for feat_1 in tqdm(cols):\n",
    "    if (feat_1 in dup):\n",
    "        continue\n",
    "    for feat_2 in cols.drop(feat_1):\n",
    "        if (feat_2 in dup):\n",
    "            continue\n",
    "        if (train_df[feat_1].equals(train_df[feat_2])):\n",
    "            train_df.drop(feat_2,inplace=True,axis=1)\n",
    "            test_df.drop(feat_2,inplace=True,axis=1)\n",
    "            dup.append(feat_2)\n",
    "\n",
    "#Drop Constant Features\n",
    "for feat in tqdm(test_df.columns):\n",
    "    if ((len(train_df[feat].value_counts().keys()) == 1) | (len(test_df[feat].value_counts().keys()) == 1)):\n",
    "        train_df.drop(feat,inplace=True,axis=1)\n",
    "        test_df.drop(feat,inplace=True,axis=1)\n",
    "        \n",
    "print('Features After Dropping: ', train_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.fillna(train_df.median())\n",
    "test_df = test_df.fillna(test_df.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_params = {'depth': 8, 'iterations': 5000, 'learning_rate': 0.0164391346853785,\n",
    "             'task_type':'GPU','reg_lambda':21.97780539780917,'verbose':0}\n",
    "cb = CatBoostClassifier(**cb_params, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Validating...')\n",
    "\n",
    "X = train_df.drop('target',axis=1).values\n",
    "y = train_df['target'].values\n",
    "\n",
    "scores = []                  \n",
    "for fold, (train_index, test_index) in enumerate(StratifiedKFold(n_splits=5).split(X, y)):\n",
    "    X_Train, X_Test = X[train_index], X[test_index]\n",
    "    y_Train, y_Test = y[train_index], y[test_index]\n",
    "    cb.fit(X_Train,y_Train)\n",
    "    y_pred = cb.predict_proba(X_Test)[:,1]\n",
    "    scores.append(roc_auc_score(y_Test,y_pred))\n",
    "    print(scores[-1])\n",
    "\n",
    "print(\"\\nMean:\",np.mean(scores),\"\\nSTD: \", np.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the Features Importances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imps = plotImp(cb,train_df.drop('target',axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Features with 0 importance\n",
    "useless_features = imps[imps['Value'] == 0]['Feature'].values\n",
    "train_df.drop(useless_features,inplace=True,axis=1)\n",
    "test_df.drop(useless_features,inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop('target',axis=1)\n",
    "y = train_df['target']\n",
    "\n",
    "cb.fit(X,y)\n",
    "test_df['target'] = cb.predict_proba(test_df)[:,1]\n",
    "\n",
    "submission = pd.DataFrame({\"ID\": ID ,\"Target\": test_df.target.values})\n",
    "submission.to_csv('FraudDetectionSubmission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
